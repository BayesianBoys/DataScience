{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit ('ds': conda)",
   "metadata": {
    "interpreter": {
     "hash": "11040ac611af7a41bf773c65026ddf042e9325b97457a3bcfe266d29b2dade2a"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Week 2: Model Selection & Curse of Dimensionality"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Your task for today is to create the __most generalizable__ model from datasets of different sizes. \n",
    "\n",
    "We will use 3 partitions of the [Titanic](https://www.kaggle.com/c/titanic) dataset of size 100, 400, and 891. This is a fairly straightforward binary classification task with the goal of predicting _who survived_0 when the Titanic sunk. \n",
    "\n",
    "The dataset has the following columns: \n",
    "\n",
    "| Variable | Definition                                 | Key                                            |   |   |\n",
    "|:----------|:--------------------------------------------|:------------------------------------------------|---|---|\n",
    "| Survival | Survival                                   | 0 = No, 1 = Yes                                |   |   |\n",
    "| Pclass   | Ticket class                               | 1 = 1st, 2 = 2nd, 3 = 3rd                      |   |   |\n",
    "| Sex      | Sex                                        |                                                |   |   |\n",
    "| Age      | Age in years                               |                                                |   |   |\n",
    "| Sibsp    | # of siblings / spouses aboard the Titanic |                                                |   |   |\n",
    "| Parch    | # of parents / children aboard the Titanic |                                                |   |   |\n",
    "| Ticket   | Ticket number                              |                                                |   |   |\n",
    "| Fare     | Passenger fare                             |                                                |   |   |\n",
    "| Cabin    | Cabin number                               |                                                |   |   |\n",
    "| Embarked | Port of Embarkation                        | C = Cherbourg, Q = Queenstown, S = Southampton |   |   |\n",
    "\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "There are some pecularities in the data: some columns contain missing values, some are redundant, and some might only be useful with feature engineering.\n",
    "\n",
    "__Exercise__:\n",
    "\n",
    "The following shows a simple example of fitting a logistic regression model to the data with 400 training examples.\n",
    "\n",
    "- Run the code and discuss ways to improve it"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pandas'",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-7db1c3503715>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinear_model\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mLogisticRegression\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mconfusion_matrix\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mroc_auc_score\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'pandas'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_folder = \"titanic\" # set to the name of the folder where you keep the data\n",
    "test = pd.read_csv(os.path.join(data_folder, \"test.csv\"))\n",
    "train = pd.read_csv(os.path.join(data_folder, \"train_400.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's take a quick look at the data\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Are there missing values in the train set?\n",
    "train.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# And the test set\n",
    "test.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Okay, lots of missing age values. Filling them with the mean value of the column\n",
    "train[\"Age\"] = train[\"Age\"].fillna(train[\"Age\"].mean())\n",
    "test[\"Age\"] = test[\"Age\"].fillna(train[\"Age\"].mean())\n",
    "\n",
    "# 1 missing Fare in test, filling with the mean\n",
    "test[\"Fare\"] = test[\"Fare\"].fillna(train[\"Fare\"].mean())\n",
    "\n",
    "# Mean imputation is very naive - can you think of better ways to impute the missing values?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's see if it worked\n",
    "train.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sklearn does not like columns with  categorical values\n",
    "# make them binary dummy variables instead\n",
    "train = pd.get_dummies(train, columns=[\"Pclass\", \"Embarked\", \"Sex\"])\n",
    "test =  pd.get_dummies(test, columns=[\"Pclass\", \"Embarked\", \"Sex\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The Ticket, PassengerId, Name, and Cabin column seem like they might be problematic\n",
    "# Let's check how many unique values they contain\n",
    "print(f\"N. of rows: {len(train)}\")\n",
    "\n",
    "for col in [\"Ticket\", \"PassengerId\", \"Name\", \"Cabin\"]:\n",
    "    print(f\"Proportion of unique values in {col}: {len(train[col].unique()) / len(train)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PassengerId, Name, and Ticket are practically unique for each individual and thus unusable for predictions\n",
    "# Cabin has a lot of missing values and a lot of unique values. Dropping the columns\n",
    "\n",
    "uninformative_cols = [\"PassengerId\", \"Name\", \"Ticket\", \"Cabin\"]\n",
    "train = train.drop(columns=uninformative_cols)\n",
    "test = test.drop(columns=uninformative_cols)\n",
    "\n",
    "# Could Cabin be made informative with some feature engineering?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a good old logistic regression model based on the remaining columns\n",
    "model = LogisticRegression()\n",
    "# Make subset of training data containing everything except the label\n",
    "X = train.loc[:, train.columns != \"Survived\"]\n",
    "# Make subset containing only the label\n",
    "Y = train[\"Survived\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # Fit model on training data\n",
    " model.fit(X, Y)\n",
    " # See how well the model does on the training data\n",
    " yhat = model.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Accuracy on train data: {accuracy_score(Y, yhat)}\")\n",
    "confusion_matrix(Y, yhat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the model on the testing set\n",
    "X_test = test.loc[:, train.columns != \"Survived\"]\n",
    "Y_test = test[\"Survived\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat_test = model.predict(X_test)\n",
    "print(f\"Accuracy on test data: {accuracy_score(Y_test, yhat_test)}\")\n",
    "confusion_matrix(Y_test, yhat_test)"
   ]
  },
  {
   "source": [
    ".. That was not very impressive. Our expectation of performance was horribly miscalibrated, as we fared much worse on the test set than on our training set. Our model also seems to overpredict survival on the test data.\n",
    "\n",
    "Now it's your turn to do better\n",
    "\n",
    "__Exercises__:\n",
    "\n",
    "Discuss:\n",
    "\n",
    "- How can you get a better estimate of the out-of-sample performance?\n",
    "- How can you reduce overfitting to the training data?\n",
    "- Do you need different strategies for model creation for the different sizes of dataset?\n",
    "    - If so, what would you do differently?\n",
    "\n",
    "Code:\n",
    "\n",
    "- For each partition (i.e. each dataset) create at least 3 different models that you expect to generalize well. Evaluate them on the training sample using some form of model selection (cross-validated performance, held-out data, information criteria etc.) and choose one to test on the testing set. Your goal is to create the best performing, most well-calibrated model, ie. training performance should be close to testing performance (and performance should of course be high!). \n",
    "- Test how good performance you can get on the small datasets with clever optimization and regularization.\n",
    "\n",
    "For next time:\n",
    "\n",
    "- In your study groups, prepare a 3-5 min presentation on something interesting about your solution: Did you create some cool functions for preprocessing, test an exciting new type of model, set everything up to be run from the command line, or perhaps you're just really excited about the performance of your model. No need for slideshows, but feel free to show code.\n",
    "\n",
    "---\n",
    "\n",
    "Tips to get started:\n",
    "- Visualization can often be a good way to get started: how is the distribution of the variables? are any of them highly correlated?\n",
    "- Instead of training and testing on the whole training data, implement a form of cross-validation ([sk-learn makes it easy](https://scikit-learn.org/stable/modules/cross_validation.html#computing-cross-validated-metrics))\n",
    "- Remember ridge regularization from last week? [Try it](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.RidgeClassifier.html)\n",
    "- Check out a list of models in sk-learn [here](https://scikit-learn.org/stable/supervised_learning.html)\n",
    "- Lost or out of ideas? Take some inspiration from entries in the [original Kaggle challenge](https://www.kaggle.com/c/titanic/code)\n",
    "\n",
    "Things to try:\n",
    "- You might be able to get more information out of the predictors if you do some feature engineering. Perhaps add a column indicating whether the person had a cabin or not, or one calculating the family size?\n",
    "- Calculating information criteriais not entirely straight-forward in sk-learn. [This tutorial](https://machinelearningmastery.com/probabilistic-model-selection-measures/) might help\n",
    "- The outcome (survival) is not completely balanced. Over-/under-sampling might help\n",
    "- Ensemble models often generalize better than single models. Try one of the methods [here](https://scikit-learn.org/stable/modules/ensemble.html)\n",
    "- Don't feel restricted to sk-learn. Feel to make a Bayesian model in [PyMC3](https://github.com/pymc-devs/pymc3) or any other framework you want\n",
    "- High-performance interpretable models are all the rage right now. [Try one of them!](https://github.com/interpretml/interpret) \n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}