{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit ('ds': conda)",
   "metadata": {
    "interpreter": {
     "hash": "11040ac611af7a41bf773c65026ddf042e9325b97457a3bcfe266d29b2dade2a"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Week 2: Model Selection & Curse of Dimensionality"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Your task for today is to create the __most generalizable__ model from datasets of different sizes. \n",
    "\n",
    "We will use 3 partitions of the [Titanic](https://www.kaggle.com/c/titanic) dataset of size 100, 400, and 891. This is a fairly straightforward binary classification task with the goal of predicting _who survived_0 when the Titanic sunk. \n",
    "\n",
    "The dataset has the following columns: \n",
    "\n",
    "| Variable | Definition                                 | Key                                            |   |   |\n",
    "|:----------|:--------------------------------------------|:------------------------------------------------|---|---|\n",
    "| Survival | Survival                                   | 0 = No, 1 = Yes                                |   |   |\n",
    "| Pclass   | Ticket class                               | 1 = 1st, 2 = 2nd, 3 = 3rd                      |   |   |\n",
    "| Sex      | Sex                                        |                                                |   |   |\n",
    "| Age      | Age in years                               |                                                |   |   |\n",
    "| Sibsp    | # of siblings / spouses aboard the Titanic |                                                |   |   |\n",
    "| Parch    | # of parents / children aboard the Titanic |                                                |   |   |\n",
    "| Ticket   | Ticket number                              |                                                |   |   |\n",
    "| Fare     | Passenger fare                             |                                                |   |   |\n",
    "| Cabin    | Cabin number                               |                                                |   |   |\n",
    "| Embarked | Port of Embarkation                        | C = Cherbourg, Q = Queenstown, S = Southampton |   |   |\n",
    "\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "There are some pecularities in the data: some columns contain missing values, some are redundant, and some might only be useful with feature engineering.\n",
    "\n",
    "__Exercise__:\n",
    "\n",
    "The following shows a simple example of fitting a logistic regression model to the data with 400 training examples.\n",
    "\n",
    "- Run the code and discuss ways to improve it"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, roc_auc_score\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_folder = \"titanic\" # set to the name of the folder where you keep the data\n",
    "test = pd.read_csv(os.path.join(data_folder, \"test.csv\"))\n",
    "#train = pd.read_csv(os.path.join(data_folder, \"train_100.csv\"))\n",
    "train = pd.read_csv(os.path.join(data_folder, \"train_400.csv\"))\n",
    "#train = pd.read_csv(os.path.join(data_folder, \"train_full.csv\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "PassengerId      0\n",
       "Pclass           0\n",
       "Name             0\n",
       "Sex              0\n",
       "Age             86\n",
       "SibSp            0\n",
       "Parch            0\n",
       "Ticket           0\n",
       "Fare             1\n",
       "Cabin          327\n",
       "Embarked         0\n",
       "Survived         0\n",
       "dtype: int64"
      ]
     },
     "metadata": {},
     "execution_count": 282
    }
   ],
   "source": [
    "# And the test set\n",
    "test.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Okay, lots of missing age values. Filling them with the mean value of the column\n",
    "train[\"Age\"] = train[\"Age\"].fillna(train[\"Age\"].mean())\n",
    "test[\"Age\"] = test[\"Age\"].fillna(train[\"Age\"].mean())\n",
    "\n",
    "# 1 missing Fare in test, filling with the mean\n",
    "test[\"Fare\"] = test[\"Fare\"].fillna(train[\"Fare\"].mean())\n",
    "\n",
    "# Mean imputation is very naive - can you think of better ways to impute the missing values?"
   ]
  },
  {
   "source": [
    "# Let's see if it worked\n",
    "train.isnull().sum()"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": 284,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "PassengerId      0\n",
       "Survived         0\n",
       "Pclass           0\n",
       "Name             0\n",
       "Sex              0\n",
       "Age              0\n",
       "SibSp            0\n",
       "Parch            0\n",
       "Ticket           0\n",
       "Fare             0\n",
       "Cabin          309\n",
       "Embarked         1\n",
       "dtype: int64"
      ]
     },
     "metadata": {},
     "execution_count": 284
    }
   ]
  },
  {
   "source": [
    "# Feature Engineering:\n",
    "## Cabin_binary:\n",
    "Binary, records whether logs had an NA or not \n",
    "\n",
    "## Family_size:\n",
    "Integer, sums the number of siblings and children of the passenger\n",
    "\n",
    "## Age_bin:\n",
    "Bins the different age groups into children ($age \\leq 14$), adult ($15 \\leq age \\leq 39$) and old ($40 \\leq age$ )"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binarize_cabin(df):\n",
    "   cabins = df[\"Cabin\"].values\n",
    "   df[\"Cabin_binary\"] = [1 if isinstance(cabin, str) else 0 for cabin in cabins]\n",
    "   return df\n",
    "\n",
    "def get_family_size(df):\n",
    "   siblings, children = df[\"SibSp\"].values, df[\"Parch\"].values\n",
    "   df[\"Family_size\"] = [siblings[i] + children[i] for i in range(len(siblings))]\n",
    "   return df\n",
    "\n",
    "def bin_age(age):\n",
    "   if age < 15:\n",
    "      return \"child\"\n",
    "   elif age < 40:\n",
    "      return \"adult\"\n",
    "   else:\n",
    "      return \"old\"   \n",
    "\n",
    "train, test = binarize_cabin(train), binarize_cabin(test)\n",
    "train, test = get_family_size(train), get_family_size(test)\n",
    "\n",
    "train[\"Age_bin\"] = train[\"Age\"].apply(lambda x: bin_age(x))\n",
    "test[\"Age_bin\"] = test[\"Age\"].apply(lambda x: bin_age(x))"
   ]
  },
  {
   "source": [
    "# Constructing dummy variables (splitting and binarizing)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sklearn does not like columns with  categorical values\n",
    "# make them binary dummy variables instead\n",
    "train = pd.get_dummies(train, columns=[\"Pclass\", \"Embarked\", \"Sex\", \"Age_bin\"])\n",
    "test =  pd.get_dummies(test, columns=[\"Pclass\", \"Embarked\", \"Sex\", \"Age_bin\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "   PassengerId  Survived                                               Name  \\\n",
       "0            1         0                            Braund, Mr. Owen Harris   \n",
       "1            2         1  Cumings, Mrs. John Bradley (Florence Briggs Th...   \n",
       "2            3         1                             Heikkinen, Miss. Laina   \n",
       "3            4         1       Futrelle, Mrs. Jacques Heath (Lily May Peel)   \n",
       "4            5         0                           Allen, Mr. William Henry   \n",
       "\n",
       "    Age  SibSp  Parch            Ticket     Fare Cabin  Cabin_binary  ...  \\\n",
       "0  22.0      1      0         A/5 21171   7.2500   NaN             0  ...   \n",
       "1  38.0      1      0          PC 17599  71.2833   C85             1  ...   \n",
       "2  26.0      0      0  STON/O2. 3101282   7.9250   NaN             0  ...   \n",
       "3  35.0      1      0            113803  53.1000  C123             1  ...   \n",
       "4  35.0      0      0            373450   8.0500   NaN             0  ...   \n",
       "\n",
       "   Pclass_2  Pclass_3  Embarked_C  Embarked_Q  Embarked_S  Sex_female  \\\n",
       "0         0         1           0           0           1           0   \n",
       "1         0         0           1           0           0           1   \n",
       "2         0         1           0           0           1           1   \n",
       "3         0         0           0           0           1           1   \n",
       "4         0         1           0           0           1           0   \n",
       "\n",
       "   Sex_male  Age_bin_adult  Age_bin_child  Age_bin_old  \n",
       "0         1              1              0            0  \n",
       "1         0              1              0            0  \n",
       "2         0              1              0            0  \n",
       "3         0              1              0            0  \n",
       "4         1              1              0            0  \n",
       "\n",
       "[5 rows x 22 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>PassengerId</th>\n      <th>Survived</th>\n      <th>Name</th>\n      <th>Age</th>\n      <th>SibSp</th>\n      <th>Parch</th>\n      <th>Ticket</th>\n      <th>Fare</th>\n      <th>Cabin</th>\n      <th>Cabin_binary</th>\n      <th>...</th>\n      <th>Pclass_2</th>\n      <th>Pclass_3</th>\n      <th>Embarked_C</th>\n      <th>Embarked_Q</th>\n      <th>Embarked_S</th>\n      <th>Sex_female</th>\n      <th>Sex_male</th>\n      <th>Age_bin_adult</th>\n      <th>Age_bin_child</th>\n      <th>Age_bin_old</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>0</td>\n      <td>Braund, Mr. Owen Harris</td>\n      <td>22.0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>A/5 21171</td>\n      <td>7.2500</td>\n      <td>NaN</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>1</td>\n      <td>Cumings, Mrs. John Bradley (Florence Briggs Th...</td>\n      <td>38.0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>PC 17599</td>\n      <td>71.2833</td>\n      <td>C85</td>\n      <td>1</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>1</td>\n      <td>Heikkinen, Miss. Laina</td>\n      <td>26.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>STON/O2. 3101282</td>\n      <td>7.9250</td>\n      <td>NaN</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4</td>\n      <td>1</td>\n      <td>Futrelle, Mrs. Jacques Heath (Lily May Peel)</td>\n      <td>35.0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>113803</td>\n      <td>53.1000</td>\n      <td>C123</td>\n      <td>1</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5</td>\n      <td>0</td>\n      <td>Allen, Mr. William Henry</td>\n      <td>35.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>373450</td>\n      <td>8.0500</td>\n      <td>NaN</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 22 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 287
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "source": [
    "# Drop uninformative columns"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PassengerId, Name, and Ticket are practically unique for each individual and thus unusable for predictions\n",
    "# Cabin has a lot of missing values and a lot of unique values. Dropping the columns\n",
    "\n",
    "uninformative_cols = [\"PassengerId\", \"Name\", \"Ticket\", \"Cabin\", \"Parch\", \"SibSp\", \"Embarked_C\", \"Embarked_Q\", \"Embarked_S\", \"Age\", \"Fare\"] #\n",
    "train = train.drop(columns=uninformative_cols)\n",
    "test = test.drop(columns=uninformative_cols)\n",
    "\n",
    "# Could Cabin be made informative with some feature engineering?"
   ]
  },
  {
   "source": [
    "# Readying the data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make subset of training data containing everything except the label\n",
    "X = train.loc[:, train.columns != \"Survived\"]\n",
    "# Make subset containing only the label\n",
    "Y = train[\"Survived\"]\n",
    "X_test = test.loc[:, train.columns != \"Survived\"]\n",
    "Y_test = test[\"Survived\"]"
   ]
  },
  {
   "source": [
    ".. That was not very impressive. Our expectation of performance was horribly miscalibrated, as we fared much worse on the test set than on our training set. Our model also seems to overpredict survival on the test data.\n",
    "\n",
    "Now it's your turn to do better\n",
    "\n",
    "__Exercises__:\n",
    "\n",
    "Discuss:\n",
    "\n",
    "- How can you get a better estimate of the out-of-sample performance?\n",
    "- How can you reduce overfitting to the training data?\n",
    "- Do you need different strategies for model creation for the different sizes of dataset?\n",
    "    - If so, what would you do differently?\n",
    "\n",
    "Code:\n",
    "\n",
    "- For each partition (i.e. each dataset) create at least 3 different models that you expect to generalize well. Evaluate them on the training sample using some form of model selection (cross-validated performance, held-out data, information criteria etc.) and choose one to test on the testing set. Your goal is to create the best performing, most well-calibrated model, ie. training performance should be close to testing performance (and performance should of course be high!). \n",
    "- Test how good performance you can get on the small datasets with clever optimization and regularization.\n",
    "\n",
    "For next time:\n",
    "\n",
    "- In your study groups, prepare a 3-5 min presentation on something interesting about your solution: Did you create some cool functions for preprocessing, test an exciting new type of model, set everything up to be run from the command line, or perhaps you're just really excited about the performance of your model. No need for slideshows, but feel free to show code.\n",
    "\n",
    "---\n",
    "\n",
    "Tips to get started:\n",
    "- Visualization can often be a good way to get started: how is the distribution of the variables? are any of them highly correlated?\n",
    "- Instead of training and testing on the whole training data, implement a form of cross-validation ([sk-learn makes it easy](https://scikit-learn.org/stable/modules/cross_validation.html#computing-cross-validated-metrics))\n",
    "- Remember ridge regularization from last week? [Try it](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.RidgeClassifier.html)\n",
    "- Check out a list of models in sk-learn [here](https://scikit-learn.org/stable/supervised_learning.html)\n",
    "- Lost or out of ideas? Take some inspiration from entries in the [original Kaggle challenge](https://www.kaggle.com/c/titanic/code)\n",
    "\n",
    "Things to try:\n",
    "- You might be able to get more information out of the predictors if you do some feature engineering. Perhaps add a column indicating whether the person had a cabin or not, or one calculating the family size?\n",
    "- Calculating information criteriais not entirely straight-forward in sk-learn. [This tutorial](https://machinelearningmastery.com/probabilistic-model-selection-measures/) might help\n",
    "- The outcome (survival) is not completely balanced. Over-/under-sampling might help\n",
    "- Ensemble models often generalize better than single models. Try one of the methods [here](https://scikit-learn.org/stable/modules/ensemble.html)\n",
    "- Don't feel restricted to sk-learn. Feel to make a Bayesian model in [PyMC3](https://github.com/pymc-devs/pymc3) or any other framework you want\n",
    "- High-performance interpretable models are all the rage right now. [Try one of them!](https://github.com/interpretml/interpret) \n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# Main Function\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      " 20%|██        | 1/5 [00:00<00:00,  7.21it/s]mean = 0.8206382543085313, sd = 0.04115697646459375\n",
      " 40%|████      | 2/5 [00:02<00:03,  1.20s/it]mean = 0.8262494703484051, sd = 0.05232619351341493\n",
      " 80%|████████  | 4/5 [00:44<00:12, 12.23s/it]mean = 0.8181161826762116, sd = 0.05640485025761589\n",
      "mean = 0.8140810780506366, sd = 0.05005694365385111\n",
      "100%|██████████| 5/5 [00:46<00:00,  9.28s/it]mean = 0.822712625941947, sd = 0.062425634235132636\n",
      "\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{LogisticRegression(): {'acc_train': 0.825,\n",
       "  'acc_test': 0.9832535885167464,\n",
       "  'mean_cross': 0.8206382543085313,\n",
       "  'std_cross': 0.04115697646459375,\n",
       "  'conf_mat': array([[215,  28],\n",
       "         [ 42, 115]], dtype=int64)},\n",
       " RandomForestClassifier(n_estimators=60, n_jobs=-1, random_state=42): {'acc_train': 0.87,\n",
       "  'acc_test': 0.9449760765550239,\n",
       "  'mean_cross': 0.8262494703484051,\n",
       "  'std_cross': 0.05232619351341493,\n",
       "  'conf_mat': array([[223,  20],\n",
       "         [ 32, 125]], dtype=int64)},\n",
       " MLPClassifier(alpha=0.001, hidden_layer_sizes=(1000, 100)): {'acc_train': 0.8675,\n",
       "  'acc_test': 0.9234449760765551,\n",
       "  'mean_cross': 0.8181161826762116,\n",
       "  'std_cross': 0.05640485025761589,\n",
       "  'conf_mat': array([[219,  24],\n",
       "         [ 29, 128]], dtype=int64)},\n",
       " RidgeClassifier(): {'acc_train': 0.82,\n",
       "  'acc_test': 0.9952153110047847,\n",
       "  'mean_cross': 0.8140810780506366,\n",
       "  'std_cross': 0.05005694365385111,\n",
       "  'conf_mat': array([[211,  32],\n",
       "         [ 40, 117]], dtype=int64)},\n",
       " AdaBoostClassifier(): {'acc_train': 0.84,\n",
       "  'acc_test': 0.9760765550239234,\n",
       "  'mean_cross': 0.822712625941947,\n",
       "  'std_cross': 0.062425634235132636,\n",
       "  'conf_mat': array([[216,  27],\n",
       "         [ 37, 120]], dtype=int64)}}"
      ]
     },
     "metadata": {},
     "execution_count": 290
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.linear_model import RidgeClassifier\n",
    "\n",
    "def get_cross(model_inst, X, Y, cv = 8, scoring = \"f1_weighted\"):\n",
    "    model = model_inst\n",
    "    model.fit(X, Y)\n",
    "    yhat = model.predict(X)\n",
    "    acc_train = accuracy_score(Y, yhat)\n",
    "    yhat_test = model.predict(X_test)\n",
    "    acc_test = accuracy_score(Y_test, yhat_test)\n",
    "    scores = cross_val_score(model_inst, X, Y, cv=8, scoring = \"f1_weighted\")\n",
    "    print(f\"mean = {np.mean(scores)}, sd = {np.std(scores)}\")\n",
    "    return {\"acc_train\": acc_train, \"acc_test\": acc_test, \"mean_cross\": np.mean(scores), \"std_cross\": np.std(scores), \"conf_mat\": confusion_matrix(Y, yhat)}\n",
    "\n",
    "### NESTED DICT APPROACH:\n",
    "\n",
    "list_of_models = [LogisticRegression(), RandomForestClassifier(n_estimators = 60, n_jobs = -1, random_state = 42), MLPClassifier(solver = \"adam\", alpha = 0.001, hidden_layer_sizes=(1000,100)), RidgeClassifier(), AdaBoostClassifier()]\n",
    "\n",
    "model_performance = {i:get_cross(i, X, Y) for i in list_of_models}\n",
    "\n",
    "model_performance"
   ]
  },
  {
   "source": [
    "## But obviously, you can't interpret these and look under the hood...\n",
    "# OR CAN YOU?!\n",
    "Introducing glassbox modeling"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": "<!-- http://127.0.0.1:7001/2626499929376/ -->\n<iframe src=\"http://127.0.0.1:7001/2626499929376/\" width=100% height=800 frameBorder=\"0\"></iframe>"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": "<!-- http://127.0.0.1:7001/2626499929568/ -->\n<iframe src=\"http://127.0.0.1:7001/2626499929568/\" width=100% height=800 frameBorder=\"0\"></iframe>"
     },
     "metadata": {}
    }
   ],
   "source": [
    "from interpret.glassbox import LogisticRegression, ExplainableBoostingClassifier\n",
    "#from interpret.greybox import DecisionTreeClassifier\n",
    "from interpret import show\n",
    "\n",
    "ebm_log = LogisticRegression()\n",
    "ebm_log.fit(X, Y)\n",
    "\n",
    "emb_exp = ExplainableBoostingClassifier()\n",
    "emb_exp.fit(X,Y)\n",
    "\n",
    "ebm_global = emb_exp.explain_global()\n",
    "show(ebm_global)\n",
    "\n",
    "ebm_global = ebm_log.explain_global()\n",
    "show(ebm_global)\n",
    "\n",
    "# or substitute with LogisticRegression, DecisionTreeClassifier, RuleListClassifier, ...\n",
    "# EBM supports pandas dataframes, numpy arrays, and handles \"string\" data natively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Accuracy on test data: 0.9138755980861244\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([[237,  29],\n",
       "       [  7, 145]], dtype=int64)"
      ]
     },
     "metadata": {},
     "execution_count": 292
    }
   ],
   "source": [
    "y_hat_test = emb_exp.predict(X_test)\n",
    "print(f\"Accuracy on test data: {accuracy_score(Y_test, yhat_test)}\")\n",
    "confusion_matrix(Y_test, yhat_test)"
   ]
  }
 ]
}