{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit ('datas': conda)",
   "metadata": {
    "interpreter": {
     "hash": "9f85d914290606a323cc56de9435b7942af32f60051201343944f78468ca5820"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Week 2: Model Selection & Curse of Dimensionality"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Your task for today is to create the __most generalizable__ model from datasets of different sizes. \n",
    "\n",
    "We will use 3 partitions of the [Titanic](https://www.kaggle.com/c/titanic) dataset of size 100, 400, and 891. This is a fairly straightforward binary classification task with the goal of predicting _who survived_0 when the Titanic sunk. \n",
    "\n",
    "The dataset has the following columns: \n",
    "\n",
    "| Variable | Definition                                 | Key                                            |   |   |\n",
    "|:----------|:--------------------------------------------|:------------------------------------------------|---|---|\n",
    "| Survival | Survival                                   | 0 = No, 1 = Yes                                |   |   |\n",
    "| Pclass   | Ticket class                               | 1 = 1st, 2 = 2nd, 3 = 3rd                      |   |   |\n",
    "| Sex      | Sex                                        |                                                |   |   |\n",
    "| Age      | Age in years                               |                                                |   |   |\n",
    "| Sibsp    | # of siblings / spouses aboard the Titanic |                                                |   |   |\n",
    "| Parch    | # of parents / children aboard the Titanic |                                                |   |   |\n",
    "| Ticket   | Ticket number                              |                                                |   |   |\n",
    "| Fare     | Passenger fare                             |                                                |   |   |\n",
    "| Cabin    | Cabin number                               |                                                |   |   |\n",
    "| Embarked | Port of Embarkation                        | C = Cherbourg, Q = Queenstown, S = Southampton |   |   |\n",
    "\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "There are some pecularities in the data: some columns contain missing values, some are redundant, and some might only be useful with feature engineering.\n",
    "\n",
    "__Exercise__:\n",
    "\n",
    "The following shows a simple example of fitting a logistic regression model to the data with 400 training examples.\n",
    "\n",
    "- Run the code and discuss ways to improve it"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_folder = \"titanic\" # set to the name of the folder where you keep the data\n",
    "test = pd.read_csv(os.path.join(data_folder, \"test.csv\"))\n",
    "train = pd.read_csv(os.path.join(data_folder, \"train_full.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "   PassengerId  Survived  Pclass  \\\n",
       "0            1         0       3   \n",
       "1            2         1       1   \n",
       "2            3         1       3   \n",
       "3            4         1       1   \n",
       "4            5         0       3   \n",
       "\n",
       "                                                Name     Sex   Age  SibSp  \\\n",
       "0                            Braund, Mr. Owen Harris    male  22.0      1   \n",
       "1  Cumings, Mrs. John Bradley (Florence Briggs Th...  female  38.0      1   \n",
       "2                             Heikkinen, Miss. Laina  female  26.0      0   \n",
       "3       Futrelle, Mrs. Jacques Heath (Lily May Peel)  female  35.0      1   \n",
       "4                           Allen, Mr. William Henry    male  35.0      0   \n",
       "\n",
       "   Parch            Ticket     Fare Cabin Embarked  \n",
       "0      0         A/5 21171   7.2500   NaN        S  \n",
       "1      0          PC 17599  71.2833   C85        C  \n",
       "2      0  STON/O2. 3101282   7.9250   NaN        S  \n",
       "3      0            113803  53.1000  C123        S  \n",
       "4      0            373450   8.0500   NaN        S  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>PassengerId</th>\n      <th>Survived</th>\n      <th>Pclass</th>\n      <th>Name</th>\n      <th>Sex</th>\n      <th>Age</th>\n      <th>SibSp</th>\n      <th>Parch</th>\n      <th>Ticket</th>\n      <th>Fare</th>\n      <th>Cabin</th>\n      <th>Embarked</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>0</td>\n      <td>3</td>\n      <td>Braund, Mr. Owen Harris</td>\n      <td>male</td>\n      <td>22.0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>A/5 21171</td>\n      <td>7.2500</td>\n      <td>NaN</td>\n      <td>S</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>1</td>\n      <td>1</td>\n      <td>Cumings, Mrs. John Bradley (Florence Briggs Th...</td>\n      <td>female</td>\n      <td>38.0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>PC 17599</td>\n      <td>71.2833</td>\n      <td>C85</td>\n      <td>C</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>1</td>\n      <td>3</td>\n      <td>Heikkinen, Miss. Laina</td>\n      <td>female</td>\n      <td>26.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>STON/O2. 3101282</td>\n      <td>7.9250</td>\n      <td>NaN</td>\n      <td>S</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4</td>\n      <td>1</td>\n      <td>1</td>\n      <td>Futrelle, Mrs. Jacques Heath (Lily May Peel)</td>\n      <td>female</td>\n      <td>35.0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>113803</td>\n      <td>53.1000</td>\n      <td>C123</td>\n      <td>S</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5</td>\n      <td>0</td>\n      <td>3</td>\n      <td>Allen, Mr. William Henry</td>\n      <td>male</td>\n      <td>35.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>373450</td>\n      <td>8.0500</td>\n      <td>NaN</td>\n      <td>S</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 170
    }
   ],
   "source": [
    "# Let's take a quick look at the data\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "PassengerId      0\n",
       "Survived         0\n",
       "Pclass           0\n",
       "Name             0\n",
       "Sex              0\n",
       "Age            177\n",
       "SibSp            0\n",
       "Parch            0\n",
       "Ticket           0\n",
       "Fare             0\n",
       "Cabin          687\n",
       "Embarked         2\n",
       "dtype: int64"
      ]
     },
     "metadata": {},
     "execution_count": 171
    }
   ],
   "source": [
    "# Are there missing values in the train set?\n",
    "train.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "PassengerId      0\n",
       "Pclass           0\n",
       "Name             0\n",
       "Sex              0\n",
       "Age             86\n",
       "SibSp            0\n",
       "Parch            0\n",
       "Ticket           0\n",
       "Fare             1\n",
       "Cabin          327\n",
       "Embarked         0\n",
       "Survived         0\n",
       "dtype: int64"
      ]
     },
     "metadata": {},
     "execution_count": 172
    }
   ],
   "source": [
    "# And the test set\n",
    "test.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Okay, lots of missing age values. Filling them with the mean value of the column\n",
    "train[\"Age\"] = train[\"Age\"].fillna(train[\"Age\"].median())\n",
    "test[\"Age\"] = test[\"Age\"].fillna(train[\"Age\"].median())\n",
    "\n",
    "# 1 missing Fare in test, filling with the mean\n",
    "test[\"Fare\"] = test[\"Fare\"].fillna(train[\"Fare\"].median())\n",
    "\n",
    "# Mean imputation is very naive - can you think of better ways to impute the missing values?"
   ]
  },
  {
   "source": [
    "Lav noget gøgl med nearest neighbor eller grouped means"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "PassengerId      0\n",
       "Survived         0\n",
       "Pclass           0\n",
       "Name             0\n",
       "Sex              0\n",
       "Age              0\n",
       "SibSp            0\n",
       "Parch            0\n",
       "Ticket           0\n",
       "Fare             0\n",
       "Cabin          687\n",
       "Embarked         2\n",
       "dtype: int64"
      ]
     },
     "metadata": {},
     "execution_count": 174
    }
   ],
   "source": [
    "# Let's see if it worked\n",
    "train.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sklearn does not like columns with  categorical values\n",
    "# make them binary dummy variables instead\n",
    "train = pd.get_dummies(train, columns=[\"Pclass\", \"Embarked\", \"Sex\"])\n",
    "test =  pd.get_dummies(test, columns=[\"Pclass\", \"Embarked\", \"Sex\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "   PassengerId  Survived                                               Name  \\\n",
       "0            1         0                            Braund, Mr. Owen Harris   \n",
       "1            2         1  Cumings, Mrs. John Bradley (Florence Briggs Th...   \n",
       "2            3         1                             Heikkinen, Miss. Laina   \n",
       "3            4         1       Futrelle, Mrs. Jacques Heath (Lily May Peel)   \n",
       "4            5         0                           Allen, Mr. William Henry   \n",
       "\n",
       "    Age  SibSp  Parch            Ticket     Fare Cabin  Pclass_1  Pclass_2  \\\n",
       "0  22.0      1      0         A/5 21171   7.2500   NaN         0         0   \n",
       "1  38.0      1      0          PC 17599  71.2833   C85         1         0   \n",
       "2  26.0      0      0  STON/O2. 3101282   7.9250   NaN         0         0   \n",
       "3  35.0      1      0            113803  53.1000  C123         1         0   \n",
       "4  35.0      0      0            373450   8.0500   NaN         0         0   \n",
       "\n",
       "   Pclass_3  Embarked_C  Embarked_Q  Embarked_S  Sex_female  Sex_male  \n",
       "0         1           0           0           1           0         1  \n",
       "1         0           1           0           0           1         0  \n",
       "2         1           0           0           1           1         0  \n",
       "3         0           0           0           1           1         0  \n",
       "4         1           0           0           1           0         1  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>PassengerId</th>\n      <th>Survived</th>\n      <th>Name</th>\n      <th>Age</th>\n      <th>SibSp</th>\n      <th>Parch</th>\n      <th>Ticket</th>\n      <th>Fare</th>\n      <th>Cabin</th>\n      <th>Pclass_1</th>\n      <th>Pclass_2</th>\n      <th>Pclass_3</th>\n      <th>Embarked_C</th>\n      <th>Embarked_Q</th>\n      <th>Embarked_S</th>\n      <th>Sex_female</th>\n      <th>Sex_male</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>0</td>\n      <td>Braund, Mr. Owen Harris</td>\n      <td>22.0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>A/5 21171</td>\n      <td>7.2500</td>\n      <td>NaN</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>1</td>\n      <td>Cumings, Mrs. John Bradley (Florence Briggs Th...</td>\n      <td>38.0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>PC 17599</td>\n      <td>71.2833</td>\n      <td>C85</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>1</td>\n      <td>Heikkinen, Miss. Laina</td>\n      <td>26.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>STON/O2. 3101282</td>\n      <td>7.9250</td>\n      <td>NaN</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4</td>\n      <td>1</td>\n      <td>Futrelle, Mrs. Jacques Heath (Lily May Peel)</td>\n      <td>35.0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>113803</td>\n      <td>53.1000</td>\n      <td>C123</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5</td>\n      <td>0</td>\n      <td>Allen, Mr. William Henry</td>\n      <td>35.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>373450</td>\n      <td>8.0500</td>\n      <td>NaN</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 176
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "N. of rows: 891\nProportion of unique values in Ticket: 0.7643097643097643\nProportion of unique values in PassengerId: 1.0\nProportion of unique values in Name: 1.0\nProportion of unique values in Cabin: 0.16610549943883277\n"
     ]
    }
   ],
   "source": [
    "# The Ticket, PassengerId, Name, and Cabin column seem like they might be problematic\n",
    "# Let's check how many unique values they contain\n",
    "print(f\"N. of rows: {len(train)}\")\n",
    "\n",
    "for col in [\"Ticket\", \"PassengerId\", \"Name\", \"Cabin\"]:\n",
    "    print(f\"Proportion of unique values in {col}: {len(train[col].unique()) / len(train)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PassengerId, Name, and Ticket are practically unique for each individual and thus unusable for predictions\n",
    "# Cabin has a lot of missing values and a lot of unique values. Dropping the columns\n",
    "\n",
    "uninformative_cols = [\"PassengerId\", \"Name\", \"Ticket\", \"Cabin\"]\n",
    "train = train.drop(columns=uninformative_cols)\n",
    "test = test.drop(columns=uninformative_cols)\n",
    "\n",
    "uninformative_cols2 = [\"Embarked_Q\", \"Embarked_C\", \"Embarked_S\", \"Parch\", \"SibSp\"]\n",
    "train2 = train.drop(columns=uninformative_cols2)\n",
    "test2 = test.drop(columns=uninformative_cols2)\n",
    "\n",
    "# Could Cabin be made informative with some feature engineering?\n",
    "#Måske cabin locations kan bruges som predictors - binary cabin eller ej?"
   ]
  },
  {
   "source": [
    "Fjern dårlige predictors og få mega god test accuracy - Embarked, Siblings/family gør modellen ringere - curse of dimensionality?"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a good old logistic regression model based on the remaining columns\n",
    "model = LogisticRegression()\n",
    "# Make subset of training data containing everything except the label\n",
    "X = train2.loc[:, train2.columns != \"Survived\"]\n",
    "# Make subset containing only the label\n",
    "Y = train2[\"Survived\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    " # Fit model on training data\n",
    " model.fit(X, Y)\n",
    " # See how well the model does on the training data\n",
    " yhat = model.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Accuracy on train data: 0.7968574635241302\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([[470,  79],\n",
       "       [102, 240]], dtype=int64)"
      ]
     },
     "metadata": {},
     "execution_count": 181
    }
   ],
   "source": [
    "print(f\"Accuracy on train data: {accuracy_score(Y, yhat)}\")\n",
    "confusion_matrix(Y, yhat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the model on the testing set\n",
    "X_test = test2.loc[:, train2.columns != \"Survived\"]\n",
    "Y_test = test2[\"Survived\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Index(['Survived', 'Age', 'Fare', 'Pclass_1', 'Pclass_2', 'Pclass_3',\n",
       "       'Sex_female', 'Sex_male'],\n",
       "      dtype='object')"
      ]
     },
     "metadata": {},
     "execution_count": 185
    }
   ],
   "source": [
    "train2.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.4285495643554833"
      ]
     },
     "metadata": {},
     "execution_count": 196
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Accuracy on test data: 0.9401913875598086\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([[264,   2],\n",
       "       [ 23, 129]], dtype=int64)"
      ]
     },
     "metadata": {},
     "execution_count": 184
    }
   ],
   "source": [
    "yhat_test = model.predict(X_test)\n",
    "print(f\"Accuracy on test data: {accuracy_score(Y_test, yhat_test)}\")\n",
    "confusion_matrix(Y_test, yhat_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "1    410\n",
       "0      8\n",
       "dtype: int64"
      ]
     },
     "metadata": {},
     "execution_count": 46
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\r\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\r\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\r\n<!-- Created with matplotlib (https://matplotlib.org/) -->\r\n<svg height=\"231.84pt\" version=\"1.1\" viewBox=\"0 0 245.518125 231.84\" width=\"245.518125pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\r\n <metadata>\r\n  <rdf:RDF xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\r\n   <cc:Work>\r\n    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\r\n    <dc:date>2021-02-09T14:54:43.648644</dc:date>\r\n    <dc:format>image/svg+xml</dc:format>\r\n    <dc:creator>\r\n     <cc:Agent>\r\n      <dc:title>Matplotlib v3.3.4, https://matplotlib.org/</dc:title>\r\n     </cc:Agent>\r\n    </dc:creator>\r\n   </cc:Work>\r\n  </rdf:RDF>\r\n </metadata>\r\n <defs>\r\n  <style type=\"text/css\">*{stroke-linecap:butt;stroke-linejoin:round;}</style>\r\n </defs>\r\n <g id=\"figure_1\">\r\n  <g id=\"patch_1\">\r\n   <path d=\"M 0 231.84 \r\nL 245.518125 231.84 \r\nL 245.518125 0 \r\nL 0 0 \r\nz\r\n\" style=\"fill:none;\"/>\r\n  </g>\r\n  <g id=\"axes_1\">\r\n   <g id=\"patch_2\">\r\n    <path d=\"M 216.574125 115.92 \r\nC 216.574125 102.021794 213.242859 88.323929 206.860371 75.977919 \r\nC 200.477884 63.631909 191.228179 52.993685 179.888836 44.957552 \r\nC 168.549493 36.92142 155.447421 31.719058 141.68405 29.787805 \r\nC 127.920679 27.856552 113.892802 29.252085 100.779677 33.857088 \r\nC 87.666551 38.46209 75.846223 46.143803 66.312373 56.256459 \r\nC 56.778523 66.369115 49.806007 78.621171 45.980973 91.982655 \r\nC 42.155939 105.344138 41.588661 119.429841 44.326827 133.055646 \r\nC 47.064993 146.681451 53.029664 159.454531 61.719416 170.301112 \r\nL 129.598125 115.92 \r\nL 216.574125 115.92 \r\nz\r\n\" style=\"fill:#1f77b4;\"/>\r\n   </g>\r\n   <g id=\"patch_3\">\r\n    <path d=\"M 61.719416 170.301112 \r\nC 72.983148 184.360545 88.341512 194.579111 105.660783 199.537153 \r\nC 122.980054 204.495194 141.419234 203.951956 158.416581 197.98291 \r\nC 175.413928 192.013864 190.144077 180.908857 200.56058 166.210701 \r\nC 210.977082 151.512545 216.574128 133.934959 216.574125 115.919984 \r\nL 129.598125 115.92 \r\nL 61.719416 170.301112 \r\nz\r\n\" style=\"fill:#ff7f0e;\"/>\r\n   </g>\r\n   <g id=\"matplotlib.axis_1\"/>\r\n   <g id=\"matplotlib.axis_2\">\r\n    <g id=\"text_1\">\r\n     <!-- Survived -->\r\n     <g transform=\"translate(14.798438 137.877031)rotate(-90)scale(0.1 -0.1)\">\r\n      <defs>\r\n       <path d=\"M 53.515625 70.515625 \r\nL 53.515625 60.890625 \r\nQ 47.90625 63.578125 42.921875 64.890625 \r\nQ 37.9375 66.21875 33.296875 66.21875 \r\nQ 25.25 66.21875 20.875 63.09375 \r\nQ 16.5 59.96875 16.5 54.203125 \r\nQ 16.5 49.359375 19.40625 46.890625 \r\nQ 22.3125 44.4375 30.421875 42.921875 \r\nL 36.375 41.703125 \r\nQ 47.40625 39.59375 52.65625 34.296875 \r\nQ 57.90625 29 57.90625 20.125 \r\nQ 57.90625 9.515625 50.796875 4.046875 \r\nQ 43.703125 -1.421875 29.984375 -1.421875 \r\nQ 24.8125 -1.421875 18.96875 -0.25 \r\nQ 13.140625 0.921875 6.890625 3.21875 \r\nL 6.890625 13.375 \r\nQ 12.890625 10.015625 18.65625 8.296875 \r\nQ 24.421875 6.59375 29.984375 6.59375 \r\nQ 38.421875 6.59375 43.015625 9.90625 \r\nQ 47.609375 13.234375 47.609375 19.390625 \r\nQ 47.609375 24.75 44.3125 27.78125 \r\nQ 41.015625 30.8125 33.5 32.328125 \r\nL 27.484375 33.5 \r\nQ 16.453125 35.6875 11.515625 40.375 \r\nQ 6.59375 45.0625 6.59375 53.421875 \r\nQ 6.59375 63.09375 13.40625 68.65625 \r\nQ 20.21875 74.21875 32.171875 74.21875 \r\nQ 37.3125 74.21875 42.625 73.28125 \r\nQ 47.953125 72.359375 53.515625 70.515625 \r\nz\r\n\" id=\"DejaVuSans-83\"/>\r\n       <path d=\"M 8.5 21.578125 \r\nL 8.5 54.6875 \r\nL 17.484375 54.6875 \r\nL 17.484375 21.921875 \r\nQ 17.484375 14.15625 20.5 10.265625 \r\nQ 23.53125 6.390625 29.59375 6.390625 \r\nQ 36.859375 6.390625 41.078125 11.03125 \r\nQ 45.3125 15.671875 45.3125 23.6875 \r\nL 45.3125 54.6875 \r\nL 54.296875 54.6875 \r\nL 54.296875 0 \r\nL 45.3125 0 \r\nL 45.3125 8.40625 \r\nQ 42.046875 3.421875 37.71875 1 \r\nQ 33.40625 -1.421875 27.6875 -1.421875 \r\nQ 18.265625 -1.421875 13.375 4.4375 \r\nQ 8.5 10.296875 8.5 21.578125 \r\nz\r\nM 31.109375 56 \r\nz\r\n\" id=\"DejaVuSans-117\"/>\r\n       <path d=\"M 41.109375 46.296875 \r\nQ 39.59375 47.171875 37.8125 47.578125 \r\nQ 36.03125 48 33.890625 48 \r\nQ 26.265625 48 22.1875 43.046875 \r\nQ 18.109375 38.09375 18.109375 28.8125 \r\nL 18.109375 0 \r\nL 9.078125 0 \r\nL 9.078125 54.6875 \r\nL 18.109375 54.6875 \r\nL 18.109375 46.1875 \r\nQ 20.953125 51.171875 25.484375 53.578125 \r\nQ 30.03125 56 36.53125 56 \r\nQ 37.453125 56 38.578125 55.875 \r\nQ 39.703125 55.765625 41.0625 55.515625 \r\nz\r\n\" id=\"DejaVuSans-114\"/>\r\n       <path d=\"M 2.984375 54.6875 \r\nL 12.5 54.6875 \r\nL 29.59375 8.796875 \r\nL 46.6875 54.6875 \r\nL 56.203125 54.6875 \r\nL 35.6875 0 \r\nL 23.484375 0 \r\nz\r\n\" id=\"DejaVuSans-118\"/>\r\n       <path d=\"M 9.421875 54.6875 \r\nL 18.40625 54.6875 \r\nL 18.40625 0 \r\nL 9.421875 0 \r\nz\r\nM 9.421875 75.984375 \r\nL 18.40625 75.984375 \r\nL 18.40625 64.59375 \r\nL 9.421875 64.59375 \r\nz\r\n\" id=\"DejaVuSans-105\"/>\r\n       <path d=\"M 56.203125 29.59375 \r\nL 56.203125 25.203125 \r\nL 14.890625 25.203125 \r\nQ 15.484375 15.921875 20.484375 11.0625 \r\nQ 25.484375 6.203125 34.421875 6.203125 \r\nQ 39.59375 6.203125 44.453125 7.46875 \r\nQ 49.3125 8.734375 54.109375 11.28125 \r\nL 54.109375 2.78125 \r\nQ 49.265625 0.734375 44.1875 -0.34375 \r\nQ 39.109375 -1.421875 33.890625 -1.421875 \r\nQ 20.796875 -1.421875 13.15625 6.1875 \r\nQ 5.515625 13.8125 5.515625 26.8125 \r\nQ 5.515625 40.234375 12.765625 48.109375 \r\nQ 20.015625 56 32.328125 56 \r\nQ 43.359375 56 49.78125 48.890625 \r\nQ 56.203125 41.796875 56.203125 29.59375 \r\nz\r\nM 47.21875 32.234375 \r\nQ 47.125 39.59375 43.09375 43.984375 \r\nQ 39.0625 48.390625 32.421875 48.390625 \r\nQ 24.90625 48.390625 20.390625 44.140625 \r\nQ 15.875 39.890625 15.1875 32.171875 \r\nz\r\n\" id=\"DejaVuSans-101\"/>\r\n       <path d=\"M 45.40625 46.390625 \r\nL 45.40625 75.984375 \r\nL 54.390625 75.984375 \r\nL 54.390625 0 \r\nL 45.40625 0 \r\nL 45.40625 8.203125 \r\nQ 42.578125 3.328125 38.25 0.953125 \r\nQ 33.9375 -1.421875 27.875 -1.421875 \r\nQ 17.96875 -1.421875 11.734375 6.484375 \r\nQ 5.515625 14.40625 5.515625 27.296875 \r\nQ 5.515625 40.1875 11.734375 48.09375 \r\nQ 17.96875 56 27.875 56 \r\nQ 33.9375 56 38.25 53.625 \r\nQ 42.578125 51.265625 45.40625 46.390625 \r\nz\r\nM 14.796875 27.296875 \r\nQ 14.796875 17.390625 18.875 11.75 \r\nQ 22.953125 6.109375 30.078125 6.109375 \r\nQ 37.203125 6.109375 41.296875 11.75 \r\nQ 45.40625 17.390625 45.40625 27.296875 \r\nQ 45.40625 37.203125 41.296875 42.84375 \r\nQ 37.203125 48.484375 30.078125 48.484375 \r\nQ 22.953125 48.484375 18.875 42.84375 \r\nQ 14.796875 37.203125 14.796875 27.296875 \r\nz\r\n\" id=\"DejaVuSans-100\"/>\r\n      </defs>\r\n      <use xlink:href=\"#DejaVuSans-83\"/>\r\n      <use x=\"63.476562\" xlink:href=\"#DejaVuSans-117\"/>\r\n      <use x=\"126.855469\" xlink:href=\"#DejaVuSans-114\"/>\r\n      <use x=\"167.96875\" xlink:href=\"#DejaVuSans-118\"/>\r\n      <use x=\"227.148438\" xlink:href=\"#DejaVuSans-105\"/>\r\n      <use x=\"254.931641\" xlink:href=\"#DejaVuSans-118\"/>\r\n      <use x=\"314.111328\" xlink:href=\"#DejaVuSans-101\"/>\r\n      <use x=\"375.634766\" xlink:href=\"#DejaVuSans-100\"/>\r\n     </g>\r\n    </g>\r\n   </g>\r\n   <g id=\"text_2\">\r\n    <!-- 0 -->\r\n    <g transform=\"translate(91.535332 28.410171)scale(0.1 -0.1)\">\r\n     <defs>\r\n      <path d=\"M 31.78125 66.40625 \r\nQ 24.171875 66.40625 20.328125 58.90625 \r\nQ 16.5 51.421875 16.5 36.375 \r\nQ 16.5 21.390625 20.328125 13.890625 \r\nQ 24.171875 6.390625 31.78125 6.390625 \r\nQ 39.453125 6.390625 43.28125 13.890625 \r\nQ 47.125 21.390625 47.125 36.375 \r\nQ 47.125 51.421875 43.28125 58.90625 \r\nQ 39.453125 66.40625 31.78125 66.40625 \r\nz\r\nM 31.78125 74.21875 \r\nQ 44.046875 74.21875 50.515625 64.515625 \r\nQ 56.984375 54.828125 56.984375 36.375 \r\nQ 56.984375 17.96875 50.515625 8.265625 \r\nQ 44.046875 -1.421875 31.78125 -1.421875 \r\nQ 19.53125 -1.421875 13.0625 8.265625 \r\nQ 6.59375 17.96875 6.59375 36.375 \r\nQ 6.59375 54.828125 13.0625 64.515625 \r\nQ 19.53125 74.21875 31.78125 74.21875 \r\nz\r\n\" id=\"DejaVuSans-48\"/>\r\n     </defs>\r\n     <use xlink:href=\"#DejaVuSans-48\"/>\r\n    </g>\r\n   </g>\r\n   <g id=\"text_3\">\r\n    <!-- 1 -->\r\n    <g transform=\"translate(161.298427 208.948576)scale(0.1 -0.1)\">\r\n     <defs>\r\n      <path d=\"M 12.40625 8.296875 \r\nL 28.515625 8.296875 \r\nL 28.515625 63.921875 \r\nL 10.984375 60.40625 \r\nL 10.984375 69.390625 \r\nL 28.421875 72.90625 \r\nL 38.28125 72.90625 \r\nL 38.28125 8.296875 \r\nL 54.390625 8.296875 \r\nL 54.390625 0 \r\nL 12.40625 0 \r\nz\r\n\" id=\"DejaVuSans-49\"/>\r\n     </defs>\r\n     <use xlink:href=\"#DejaVuSans-49\"/>\r\n    </g>\r\n   </g>\r\n  </g>\r\n </g>\r\n</svg>\r\n",
      "image/png": "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\n"
     },
     "metadata": {}
    }
   ],
   "source": [
    "train.Survived.value_counts().plot(kind= \"pie\")\n",
    "pd.DataFrame(yhat_test).value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0    266\n",
       "1    152\n",
       "Name: Survived, dtype: int64"
      ]
     },
     "metadata": {},
     "execution_count": 44
    }
   ],
   "source": [
    "test.Survived.value_counts()\n"
   ]
  },
  {
   "source": [
    ".. That was not very impressive. Our expectation of performance was horribly miscalibrated, as we fared much worse on the test set than on our training set. Our model also seems to overpredict survival on the test data.\n",
    "\n",
    "Now it's your turn to do better\n",
    "\n",
    "__Exercises__:\n",
    "\n",
    "Discuss:\n",
    "\n",
    "- How can you get a better estimate of the out-of-sample performance?\n",
    "- How can you reduce overfitting to the training data?\n",
    "- Do you need different strategies for model creation for the different sizes of dataset?\n",
    "    - If so, what would you do differently?\n",
    "\n",
    "Code:\n",
    "\n",
    "- For each partition (i.e. each dataset) create at least 3 different models that you expect to generalize well. Evaluate them on the training sample using some form of model selection (cross-validated performance, held-out data, information criteria etc.) and choose one to test on the testing set. Your goal is to create the best performing, most well-calibrated model, ie. training performance should be close to testing performance (and performance should of course be high!). \n",
    "- Test how good performance you can get on the small datasets with clever optimization and regularization.\n",
    "\n",
    "For next time:\n",
    "\n",
    "- In your study groups, prepare a 3-5 min presentation on something interesting about your solution: Did you create some cool functions for preprocessing, test an exciting new type of model, set everything up to be run from the command line, or perhaps you're just really excited about the performance of your model. No need for slideshows, but feel free to show code.\n",
    "\n",
    "---\n",
    "\n",
    "Tips to get started:\n",
    "- Visualization can often be a good way to get started: how is the distribution of the variables? are any of them highly correlated?\n",
    "- Instead of training and testing on the whole training data, implement a form of cross-validation ([sk-learn makes it easy](https://scikit-learn.org/stable/modules/cross_validation.html#computing-cross-validated-metrics))\n",
    "- Remember ridge regularization from last week? [Try it](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.RidgeClassifier.html)\n",
    "- Check out a list of models in sk-learn [here](https://scikit-learn.org/stable/supervised_learning.html)\n",
    "- Lost or out of ideas? Take some inspiration from entries in the [original Kaggle challenge](https://www.kaggle.com/c/titanic/code)\n",
    "\n",
    "Things to try:\n",
    "- You might be able to get more information out of the predictors if you do some feature engineering. Perhaps add a column indicating whether the person had a cabin or not, or one calculating the family size?\n",
    "- Calculating information criteriais not entirely straight-forward in sk-learn. [This tutorial](https://machinelearningmastery.com/probabilistic-model-selection-measures/) might help\n",
    "- The outcome (survival) is not completely balanced. Over-/under-sampling might help\n",
    "- Ensemble models often generalize better than single models. Try one of the methods [here](https://scikit-learn.org/stable/modules/ensemble.html)\n",
    "- Don't feel restricted to sk-learn. Feel to make a Bayesian model in [PyMC3](https://github.com/pymc-devs/pymc3) or any other framework you want\n",
    "- High-performance interpretable models are all the rage right now. [Try one of them!](https://github.com/interpretml/interpret) \n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Overfitting blev forhindret ved at fjerne irrelevante predictors\n",
    "    Denne effekt ses størst på et lille datasæt, men er stadig til stede i det store datasæt."
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ]
}